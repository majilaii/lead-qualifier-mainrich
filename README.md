# â—ˆ Hunt

**AI-Powered B2B Lead Discovery & Qualification Platform**

Discover, crawl, qualify, and research potential B2B customers using AI. Describe your ideal customer in plain English â€” Hunt finds them, scores them, and organises everything in a full-featured dashboard with pipeline management and a live map view.

---

## What It Does

```
Web App (primary):

  Chat  â”€â”€â†’  AI Query Gen  â”€â”€â†’  Exa Search  â”€â”€â†’  Crawl + Qualify  â”€â”€â†’  Dashboard
(describe)    (dual-LLM)         (find)          (stream SSE)       (map Â· pipeline Â· stats)

CLI (legacy, deprecated):

  CSV / Exa  â”€â”€â†’  Web Crawler  â”€â”€â†’  AI Qualifier  â”€â”€â†’  Deep Research  â”€â”€â†’  CSV Output
```

| Step | Module | What Happens |
|------|--------|-------------|
| **1. Discovery** | `test_exa.py` | Finds company websites matching your ideal customer profile (ICP) using [Exa AI](https://exa.ai) neural search |
| **2. Crawling** | `scraper.py` | Visits each website with a headless browser, extracts page text as markdown + takes a screenshot |
| **3. Qualification** | `intelligence.py` | LLM reads the website content + screenshot and scores 1-10 on how likely this company needs your product |
| **4. Deep Research** | `deep_research.py` | For hot leads (score 8+), crawls multiple pages and generates a sales brief: products they make, who to talk to, what to say |
| **5. Enrichment** | `enrichment.py` | Looks up contact emails/phones via Apollo.io or Hunter.io (optional, manual mode by default) |
| **6. Dashboard** | Frontend | Full pipeline view: stats, searchable leads table, detail drawer, interactive map, settings |

### Lead Tiers

Leads are automatically sorted into 3 tiers:

| Tier | Score | Action |
|------|-------|--------|
| ğŸ”¥ **Hot** | 70-100 | Ready for outreach |
| ğŸ” **Review** | 40-69 | Human review needed |
| âŒ **Rejected** | 0-39 | Not a fit (with explanation) |

---

## Dashboard

Once logged in, the full dashboard is available at `/dashboard`. It includes:

| Page | Route | Description |
|------|-------|-------------|
| **Overview** | `/dashboard` | Stats cards (total leads, hot leads, searches, enrichments), recent hunts list |
| **Hunts** | `/dashboard/hunts` | Card grid of all saved searches â€” tier breakdown, delete, click to **resume** any previous hunt with full conversation + results restored |
| **Pipeline** | `/dashboard/pipeline` | Sortable leads table with tier/text filters. Click any row to open the detail drawer (score gauge, AI reasoning, signals, red flags, status management, enrichment data) |
| **Map** | `/dashboard/map` | Split-panel interactive map â€” left list + right Mapbox GL dark map with glowing dots (red=hot, amber=review, grey=rejected), click-to-fly, popups. **Live mode:** leads pop up on the map in real-time as the pipeline qualifies them |
| **Settings** | `/dashboard/settings` | API status indicators, usage stats, account info, sign out |

### Pipeline Statuses

Each lead can be moved through a CRM-like pipeline:

`new` â†’ `contacted` â†’ `in_progress` â†’ `won` / `lost` / `archived`

---

## Quick Start

### Prerequisites

- **Python 3.9+** and **Node.js 18+** â€” or just [Docker Desktop](https://www.docker.com/products/docker-desktop/) (skips all local setup)
- **At least one LLM API key** (Kimi or OpenAI)
- **Supabase project** for auth + database (already provisioned)

### Option A: Local Development (recommended for contributors)

You need **two terminals** â€” one for the Python backend, one for the Next.js frontend.

**1. Clone & install**

```bash
git clone <repo-url>
cd lead-qualifier
```

**2. Backend setup**

```bash
cd backend
python3 -m venv venv
source venv/bin/activate           # macOS/Linux (venv\Scripts\activate on Windows)
pip install -r requirements.txt
playwright install chromium        # headless browser for web scraping

# Configure environment
cp .env.example .env
# Edit .env â€” at minimum set KIMI_API_KEY or OPENAI_API_KEY and DATABASE_URL
```

**3. Frontend setup**

```bash
cd frontend
npm install

# .env.local should already exist with Supabase keys
# If not, create it:
cat > .env.local << 'EOF'
NEXT_PUBLIC_SUPABASE_URL=https://fwtxlbjnjfzqmqqmsssb.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=<your-anon-key>
NEXT_PUBLIC_API_URL=http://localhost:8000
NEXT_PUBLIC_MAPBOX_TOKEN=<optional-mapbox-token>
EOF
```

**4. Run both servers**

```bash
# Terminal 1 â€” Backend (FastAPI on :8000)
cd backend
source venv/bin/activate
python3 -m uvicorn chat_server:app --reload --port 8000

# Terminal 2 â€” Frontend (Next.js on :3000)
cd frontend
npm run dev
```

Open **http://localhost:3000** â†’ sign up â†’ start hunting.

### Option B: Docker (recommended for deployment)

Docker bundles Python 3.12, Node 22, Playwright, Chromium, and all dependencies.

```bash
# 1. Configure API keys (one-time)
cp backend/.env.example backend/.env
# Edit backend/.env â€” add KIMI_API_KEY or OPENAI_API_KEY + DATABASE_URL

# 2. Build & start
docker compose up --build

# Open http://localhost:3000
```

**Day-to-day commands:**
```bash
docker compose up              # Start (foreground)
docker compose up -d           # Start (background)
docker compose down            # Stop everything
docker compose logs -f backend # Tail backend logs
docker compose build --no-cache # Full rebuild
```

### Chat Flow

1. **Describe** what companies you're looking for (e.g., "metal fabrication shops with CNC capabilities")
2. **Answer** 2-3 follow-up questions â€” the AI tracks readiness across: industry, company profile, technology focus, and qualifying criteria
3. **Launch Search** â€” generates semantic queries via AI, searches the web via Exa
4. **Qualify** â€” crawls each company's website and scores them with the LLM
5. **Results** â€” hot leads, needs-review, and rejected, with reasoning and signals for each
6. **Dashboard** â€” all results (including the full chat conversation) are saved to your account. Resume any previous hunt from the Hunts page â€” the conversation, search context, and qualified leads are fully restored

### Architecture & Security

The chat uses a **dual-LLM pattern** for prompt injection defense:

| Layer | What |
|-------|------|
| **Conversation LLM** | Talks to the user, asks follow-ups, extracts structured search parameters |
| **Query Generation LLM** | Takes *only* the validated structured context â€” never sees raw user text |
| **Input sanitization** | Strips injection patterns, LLM special tokens, HTML, control characters |
| **Output validation** | Generated queries are validated (count, length, category) before execution |
| **Rate limiting** | 30 requests/min per IP on the backend |

### Required API Keys

At minimum `KIMI_API_KEY` or `OPENAI_API_KEY` in `backend/.env`. For search, also add `EXA_API_KEY`. See the [Configuration](#2-configure-api-keys) section for the full list.

---

## Environment Variables

### Backend (`backend/.env`)

| Variable | Required | Description |
|----------|----------|-------------|
| `DATABASE_URL` | âœ… | PostgreSQL connection string: `postgresql+asyncpg://user:pass@host:5432/postgres` |
| `SUPABASE_URL` | âœ… | Supabase project URL (for JWT verification) |
| `KIMI_API_KEY` | âœ…* | Moonshot Kimi API key (cheapest LLM option) |
| `OPENAI_API_KEY` | âœ…* | OpenAI API key (fallback) |
| `EXA_API_KEY` | Recommended | Exa AI key for lead discovery |
| `APOLLO_API_KEY` | Optional | Contact enrichment (50 free/month) |
| `HUNTER_API_KEY` | Optional | Email finder (25 free/month) |

*At least one of `KIMI_API_KEY` or `OPENAI_API_KEY` is required.

### Frontend (`frontend/.env.local`)

| Variable | Required | Description |
|----------|----------|-------------|
| `NEXT_PUBLIC_SUPABASE_URL` | âœ… | Supabase project URL |
| `NEXT_PUBLIC_SUPABASE_ANON_KEY` | âœ… | Supabase anonymous key |
| `NEXT_PUBLIC_API_URL` | âœ… | Backend URL (default: `http://localhost:8000`) |
| `NEXT_PUBLIC_MAPBOX_TOKEN` | Optional | Mapbox GL token for the map page (falls back to free CARTO tiles) |

---

## CLI Pipeline (Legacy)

### Quick Start

> **Note:** The CLI pipeline is deprecated in favour of the web dashboard. It still works but results are not saved to the database.

### Prerequisites

- **Python 3.11+** (or just [Docker Desktop](https://www.docker.com/products/docker-desktop/) â€” skips all local setup)
- **At least one LLM API key** (see Step 2)

### 1. Clone & Set Up Environment

```bash
git clone <your-repo-url>
cd lead-qualifier

# Create virtual environment
cd backend
python -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Install browser for web crawling
playwright install chromium
```

### 2. Configure API Keys

```bash
cp backend/.env.example backend/.env
```

Edit `backend/.env` and add your keys. **You need at minimum ONE of these:**

| Key | What For | Where to Get It | Cost |
|-----|----------|-----------------|------|
| `KIMI_API_KEY` | Lead qualification (vision + text) â€” **recommended** | [platform.moonshot.cn](https://platform.moonshot.cn/) | ~Â¥0.70/1M input tokens |
| `OPENAI_API_KEY` | Lead qualification (fallback) | [platform.openai.com](https://platform.openai.com/) | $0.15/1M input tokens (gpt-4o-mini) |
| `EXA_API_KEY` | Lead discovery (find companies) â€” **optional** | [dashboard.exa.ai](https://dashboard.exa.ai/) | $10 free credit on signup |

Optional enrichment keys (skip these initially):

| Key | What For | Free Tier |
|-----|----------|-----------|
| `APOLLO_API_KEY` | Email/phone lookup | 50 credits/month |
| `HUNTER_API_KEY` | Email finder | 25 searches/month |

### 3. Prepare Your Leads

**Option A: Use Exa to discover leads automatically** (requires `EXA_API_KEY`)

```bash
cd backend
python test_exa.py --export
# Creates output/exa_leads_YYYYMMDD_HHMM_for_qualifier.csv
```

**Option B: Import from LinkedIn Sales Navigator / your own CSV**

Create `backend/input_leads.csv`:

```csv
company_name,website_url,contact_name,linkedin_profile_url
Boston Dynamics,https://www.bostondynamics.com,Marc Raibert,https://linkedin.com/in/marcraibert
Figure AI,https://www.figure.ai,Brett Adcock,
```

**Option C: Use the included sample file**

```bash
cp backend/sample_leads.csv backend/input_leads.csv
```

### 4. Run

```bash
cd backend

# Quick test with 4 sample companies (Boston Dynamics, Figure AI, Maxon, HubSpot)
python main.py --test

# Process your own leads
python main.py --input input_leads.csv

# Process with deep research on hot leads (more pages crawled, sales brief generated)
python main.py --input input_leads.csv --deep-research

# Text-only mode (no screenshots sent to LLM â€” cheaper, faster)
python main.py --input input_leads.csv --no-vision

# Start fresh, ignore previous checkpoint
python main.py --input input_leads.csv --clear-checkpoint
```

Or use the convenience script:

```bash
chmod +x backend/run.sh
cd backend
./run.sh test              # Test with sample companies
./run.sh run               # Process input_leads.csv
./run.sh run --deep        # Process with deep research
./run.sh discover          # Run Exa discovery + export CSV
./run.sh export            # Export results to Excel
./run.sh deep 'Maxon' 'https://www.maxongroup.com'  # Deep research one company
```

---

## Project Structure

```
lead-qualifier/
â”‚
â”œâ”€â”€ docker-compose.yml          # ğŸ³ Orchestrates backend + frontend containers
â”‚
â”œâ”€â”€ backend/                    # Python pipeline + API server
â”‚   â”œâ”€â”€ Dockerfile              # ğŸ³ Python 3.12 + Playwright/Chromium image
â”‚   â”œâ”€â”€ chat_server.py          # ğŸŒ FastAPI server â€” chat, pipeline, dashboard API
â”‚   â”œâ”€â”€ chat_engine.py          # ğŸ§  Dual-LLM chat engine (conversation + query gen)
â”‚   â”œâ”€â”€ config.py               # âš™ï¸  Settings: API keys, thresholds (no hardcoded ICP)
â”‚   â”œâ”€â”€ models.py               # ğŸ“¦ Pydantic data models
â”‚   â”œâ”€â”€ main.py                 # ğŸ¯ CLI pipeline orchestrator (deprecated)
â”‚   â”‚
â”‚   â”œâ”€â”€ test_exa.py             # ğŸ” Step 1: Exa AI lead discovery
â”‚   â”œâ”€â”€ scraper.py              # ğŸŒ Step 2: Web crawling (crawl4ai + Playwright)
â”‚   â”œâ”€â”€ intelligence.py         # ğŸ§  Step 3: LLM qualification (Kimi / OpenAI)
â”‚   â”œâ”€â”€ deep_research.py        # ğŸ”¬ Step 4: Multi-page research for hot leads
â”‚   â”œâ”€â”€ enrichment.py           # ğŸ“‡ Step 5: Contact enrichment (Apollo / Hunter)
â”‚   â”œâ”€â”€ export.py               # ğŸ“Š Step 6: Export to Excel / Google Sheets
â”‚   â”‚
â”‚   â”œâ”€â”€ db/                     # Database layer
â”‚   â”‚   â”œâ”€â”€ __init__.py         # SQLAlchemy async engine + session factory
â”‚   â”‚   â””â”€â”€ models.py           # ORM models: profiles, searches, qualified_leads, etc.
â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â””â”€â”€ __init__.py         # Supabase JWT verification (JWKS)
â”‚   â”‚
â”‚   â”œâ”€â”€ utils.py                # ğŸ”§ Helpers: checkpointing, cost tracking
â”‚   â”œâ”€â”€ usage.py                # ğŸ“Š Usage tracking
â”‚   â”œâ”€â”€ logging_config.py       # ğŸ“ Structured logging
â”‚   â”œâ”€â”€ run.sh                  # ğŸš€ Convenience shell script
â”‚   â”œâ”€â”€ supabase_migration.sql  # ğŸ—„ï¸  Database schema (CREATE TABLE statements)
â”‚   â”œâ”€â”€ requirements.txt        # ğŸ“¦ Python dependencies
â”‚   â”œâ”€â”€ .env.example            # ğŸ”‘ API key template â€” copy to .env
â”‚   â””â”€â”€ output/                 # ğŸ“ CLI output files (gitignored)
â”‚
â”œâ”€â”€ frontend/                   # Next.js 16 web UI
â”‚   â”œâ”€â”€ Dockerfile              # ğŸ³ Multi-stage Node 22 build
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ middleware.ts       # Auth guards (/chat/*, /dashboard/*)
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx        # Landing page
â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx      # Root layout
â”‚   â”‚   â”‚   â”œâ”€â”€ globals.css     # Theme tokens + animations
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ chat/page.tsx               # AI chat interface
â”‚   â”‚   â”‚   â”œâ”€â”€ login/page.tsx              # Login page
â”‚   â”‚   â”‚   â”œâ”€â”€ signup/page.tsx             # Signup page
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx              # Sidebar shell (nav, mobile bottom bar)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx                # Overview (stats + recent hunts)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ hunts/page.tsx          # Saved searches grid
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx            # Leads table (filter, sort, search)
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ LeadDrawer.tsx      # Detail slide-out (score, signals, status)
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ map/page.tsx            # Interactive map (Mapbox GL, live pipeline)
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ settings/page.tsx       # API status, usage, account
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â”œâ”€â”€ api/                        # Next.js API proxy routes
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chat/route.ts
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ chat/search/route.ts
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ enrich/route.ts
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ pipeline/run/route.ts
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ usage/route.ts
â”‚   â”‚   â”‚   â”‚
â”‚   â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚   â”‚       â”œâ”€â”€ Navbar.tsx              # Top nav (Dashboard link when logged in)
â”‚   â”‚   â”‚       â”œâ”€â”€ Footer.tsx
â”‚   â”‚   â”‚       â”œâ”€â”€ chat/ChatInterface.tsx  # Full chat + pipeline streaming UI
â”‚   â”‚   â”‚       â”œâ”€â”€ hunt/HuntContext.tsx     # Global state context â€” persists chat, pipeline, map across navigation
â”‚   â”‚   â”‚       â”œâ”€â”€ auth/
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ AuthGuard.tsx
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ SessionProvider.tsx
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ UserMenu.tsx
â”‚   â”‚   â”‚       â””â”€â”€ ...                     # Landing page components
â”‚   â”‚   â””â”€â”€ lib/supabase/                   # Supabase client helpers
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tsconfig.json
â”‚
â””â”€â”€ README.md
```

---

## Database Schema

Hunt uses Supabase PostgreSQL. The schema is defined in `backend/supabase_migration.sql` and the ORM models live in `backend/db/models.py`.

| Table | Purpose |
|-------|---------|
| `profiles` | User profiles (synced from Supabase Auth via trigger) |
| `searches` | Saved search sessions â€” industry, criteria, query list, lead counts, **chat messages (JSONB)** |
| `qualified_leads` | Individual leads â€” company, score, tier, reasoning, signals, geo, status |
| `enrichment_results` | Contact data (email, phone, job title) linked to leads |
| `usage_tracking` | Per-user usage logs (searches, enrichments, costs) |

**Key columns on `qualified_leads`:**

| Column | Type | Description |
|--------|------|-------------|
| `score` | Integer | AI qualification score (0-100) |
| `tier` | String | `hot` / `review` / `rejected` |
| `status` | String | Pipeline status: `new` / `contacted` / `in_progress` / `won` / `lost` / `archived` |
| `country` | String | Extracted from LLM analysis or inferred from domain TLD |
| `latitude` / `longitude` | Float | Geo coordinates for map plotting (auto-geocoded from HQ location) |
| `key_signals` | Text | JSON array of positive signals |
| `red_flags` | Text | JSON array of concerns |
| `deep_research` | Text | Multi-page sales intelligence brief |

To run migrations against Supabase:
```bash
cd backend
python3 -c "
import asyncio, asyncpg
async def migrate():
    conn = await asyncpg.connect('postgresql://postgres.YOUR_REF:YOUR_PASS@YOUR_HOST:5432/postgres')
    with open('supabase_migration.sql') as f: await conn.execute(f.read())
    print('Done')
    await conn.close()
asyncio.run(migrate())
"
```

---

## How Each Module Works

### `chat_server.py` â€” API Server

FastAPI server that powers the entire platform. All dashboard endpoints require a Supabase JWT.

| Endpoint | Auth | Description |
|----------|------|-------------|
| `POST /api/chat` | â€” | Send conversation to LLM, returns response + readiness state |
| `POST /api/chat/search` | âœ… | Generate Exa queries from structured context, execute search |
| `POST /api/pipeline/run` | âœ… | Run crawl + qualify on found companies, stream results via SSE |
| `GET /api/health` | â€” | Health check (LLM + Exa availability) |
| `GET /api/usage` | âœ… | Usage stats for the current user |
| `GET /api/dashboard/stats` | âœ… | Aggregate stats (total leads, hot, searches, enrichments) |
| `GET /api/searches` | âœ… | List all saved searches |
| `GET /api/searches/:id` | âœ… | Single search with lead counts |
| `DELETE /api/searches/:id` | âœ… | Delete a search and all its leads |
| `GET /api/leads` | âœ… | List leads (filterable by `tier`, `search_id`, sortable) |
| `GET /api/leads/geo` | âœ… | Leads with lat/lng for map plotting |
| `GET /api/leads/:id` | âœ… | Full lead detail + enrichment data |
| `PATCH /api/leads/:id/status` | âœ… | Update pipeline status (new/contacted/in_progress/won/lost/archived) |

Rate limited at 30 requests/min per IP. CORS configured for the frontend.

### `chat_engine.py` â€” Dual-LLM Chat Engine

The brain behind the chat interface. Two isolated LLM pipelines:

1. **Conversation LLM** â€” Talks to the user with a hardened system prompt. Extracts structured search parameters (industry, tech focus, criteria) through natural conversation. Outputs constrained JSON.
2. **Query Generation LLM** â€” Receives *only* the validated structured context. Generates 4-8 semantic Exa search queries. Never sees raw user input.

Falls back through: Kimi K2.5 â†’ GPT-4o-mini. Input sanitization strips prompt injection patterns, special tokens, and HTML.

### `test_exa.py` â€” Lead Discovery

Uses [Exa AI](https://exa.ai) neural search to find companies matching your ideal customer profile. Exa is like Google but understands *meaning*, not just keywords. Describe the company you want (e.g., "humanoid robot company building actuators") and it returns matching websites.

- **Input:** Natural language search queries (generated dynamically from the chat, or manual)
- **Output:** Company URLs + titles + text snippets + relevance scores
- **Does NOT qualify leads** â€” just finds them. Qualification is `intelligence.py`'s job.

### `scraper.py` â€” Web Crawling

Launches a headless Chromium browser via [crawl4ai](https://github.com/unclecode/crawl4ai) to:
- Load the company's homepage
- Extract the full page as clean Markdown text
- Capture a screenshot (for vision model analysis)
- Auto-remove popups/overlays/cookie banners

Screenshots are resized to 720px wide JPEG to keep vision API costs low.

### `intelligence.py` â€” AI Qualification â­ (Core Value)

This is where the magic happens. The LLM:
1. Reads the website markdown text
2. Optionally analyzes the screenshot (vision mode)
3. Scores the company 1-10 on how well they match the user's ICP
4. Returns structured JSON with: `confidence_score`, `hardware_type`, `industry_category`, `headquarters_location`, `reasoning`, `key_signals`, `red_flags`

Qualification prompts are built dynamically from the user's search context (industry, technology focus, qualifying criteria, disqualifiers) â€” no hardcoded industry assumptions.

**Model priority chain:**
1. **Kimi K2.5** (vision + text) â€” cheapest option, supports screenshots
2. **OpenAI GPT-4o** (vision fallback)
3. **GPT-4o-mini** (text-only fallback)
4. **Keyword matching** (zero-cost fallback if all APIs fail)

Includes a quick pre-filter that rejects obvious non-fit companies (SaaS-only, agencies, consultancies) without burning any LLM tokens.

### `deep_research.py` â€” Sales Intelligence

For hot leads (score â‰¥ 8), crawls up to 5 pages on their site and generates:
- Products they manufacture or services they provide
- Technology stack and capabilities
- Company size and production volume estimates
- Decision-maker titles to target
- A suggested pitch angle and talking points

### `enrichment.py` â€” Contact Lookup

Two modes:
- **Manual mode** (default): Flags leads for manual LinkedIn/email lookup â€” zero cost
- **API mode**: Uses Apollo.io and/or Hunter.io to find emails and phone numbers

Start with manual mode. Enable API enrichment once the pipeline is proven and you're ready to do outreach.

### `export.py` â€” Export & Share

- **Excel:** Creates `.xlsx` with separate sheets for Hot / Review / Rejected
- **Google Sheets:** Uploads directly (requires Google Cloud service account)
- **Watch mode:** Auto-syncs to Sheets when files change

---

## CLI Reference

### `backend/main.py` â€” Main Pipeline

```
cd backend
python main.py [OPTIONS]

Options:
  --test                Run test with 4 sample companies
  --input FILE          Custom input CSV path (default: input_leads.csv)
  --no-vision           Disable screenshot analysis (text-only, cheaper)
  --auto-enrich         Auto-lookup emails for hot leads (needs Apollo/Hunter keys)
  --deep-research       Multi-page analysis on hot leads (generates sales brief)
  --clear-checkpoint    Delete previous progress and start fresh
```

### `backend/test_exa.py` â€” Lead Discovery

```
cd backend
python test_exa.py [OPTIONS]

Options:
  --query N    Run only query #N (1-12), e.g. --query 1 for humanoid robots
  --export     Export results to CSV (auto-creates qualifier-compatible format)
```

### `backend/deep_research.py` â€” Single Company Research

```
cd backend
python deep_research.py <company_name> <website_url>

Example:
  python deep_research.py "Maxon Group" "https://www.maxongroup.com"
```

### `backend/export.py` â€” Export Results

```
cd backend
python export.py [excel|sheets|watch]

  excel   â†’ Creates .xlsx with Hot/Review/Rejected sheets
  sheets  â†’ Uploads to Google Sheets (needs credentials.json)
  watch   â†’ Auto-syncs on file changes
```

---

## Configuration Guide

All configuration is in `backend/config.py`. Key settings:

| Setting | Default | What It Controls |
|---------|---------|-----------------|
| `CONCURRENCY_LIMIT` | 5 | How many websites to crawl in parallel |
| `MAX_TOKENS_INPUT` | 6000 | Max tokens of website text sent to LLM per lead |
| `SCORE_HOT_LEAD` | 8 | Minimum score to be classified as a "hot" lead |
| `SCORE_REVIEW` | 4 | Minimum score for "review" bucket (below this â†’ rejected) |
| `TEXT_MODEL` | gpt-4o-mini | LLM for text-only qualification |
| `VISION_MODEL` | kimi-k2.5-thinking | LLM for vision qualification |
| `REQUEST_TIMEOUT` | 30 | Seconds before a crawl times out |

### Customizing the ICP (Ideal Customer Profile)

Hunt's qualification logic is **fully dynamic** â€” driven by the user's search context from the chat interface. There are no hardcoded industry prompts.

When a user describes their ideal customer in chat, the AI extracts structured parameters (industry, technology focus, qualifying criteria, disqualifiers) and builds LLM prompts on-the-fly for each search.

For the CLI pipeline, you can still tweak `backend/config.py`:

- **`NEGATIVE_KEYWORDS`** â€” Universal B2B negatives (SaaS, agencies, etc.) used for quick pre-filtering
- **Score thresholds** â€” `SCORE_HOT_LEAD` (default: 8), `SCORE_REVIEW` (default: 4)

---

## Cost Estimates

| Operation | Cost per unit | Notes |
|-----------|--------------|-------|
| Exa search (1 query, â‰¤25 results) | ~$0.005 | 12 queries â‰ˆ $0.06 |
| Exa content retrieval | ~$0.001/page | Included in search call |
| Kimi K2.5 qualification | ~$0.002/lead | Vision + text |
| GPT-4o-mini qualification | ~$0.001/lead | Text only |
| GPT-4o qualification | ~$0.01/lead | With vision |
| Deep research (per lead) | ~$0.005 | 3-5 pages crawled + LLM |
| Apollo enrichment | Free (50/mo) | Then $49/mo for 2,400 |
| Hunter enrichment | Free (25/mo) | Then $49/mo for 500 |

**Typical full run:** 100 leads â‰ˆ **$0.20 â€“ $0.50** with Kimi

---

## Resume & Checkpointing

The pipeline automatically saves progress. If it crashes or you stop it (`Ctrl+C`):
- Already-processed leads are skipped on re-run
- Output CSVs are appended to, not overwritten
- Checkpoint is stored in `output/.checkpoint.json`

To start completely fresh:
```bash
python main.py --clear-checkpoint
```

---

## Troubleshooting

### Chat interface not connecting to backend
Make sure both servers are running:
- Backend: `cd backend && source venv/bin/activate && python3 -m uvicorn chat_server:app --reload --port 8000`
- Frontend: `cd frontend && npm run dev`

Check `http://localhost:8000/api/health` â€” it should return `{"status": "ok", "llm_available": true, ...}`.

### Dashboard shows no data
Make sure you've run at least one search from the chat interface (`/chat`). The pipeline saves results to the database automatically. Check that `DATABASE_URL` in `backend/.env` is set correctly.

### Map page is blank
The map requires a Mapbox GL token. Set `NEXT_PUBLIC_MAPBOX_TOKEN` in `frontend/.env.local` to enable the dark-v11 map style. Leads appear on the map automatically â€” the LLM extracts company headquarters from website content and a built-in geocoder converts locations to coordinates. If no HQ is found, the system falls back to domain TLD country detection.

### Docker build fails
```bash
# Make sure Docker Desktop is running, then:
docker compose build --no-cache   # Full rebuild
```

If you see Playwright/Chromium errors in the container, the Dockerfile already handles all system deps. If it persists, try: `docker compose down && docker system prune -f && docker compose up --build`.

### Docker containers can't communicate
The frontend waits for the backend health check before starting. If the backend is unhealthy:
```bash
docker compose logs backend    # Check for API key errors
curl http://localhost:8000/api/health   # Should return {"status": "ok"}
```

### "No LLM API configured"
â†’ Add `OPENAI_API_KEY` or `KIMI_API_KEY` to your `backend/.env` file.

### Playwright / Chromium errors
```bash
playwright install chromium
# On Linux, also run:
playwright install-deps chromium
```

### Many crawl failures / timeouts
Some sites block headless browsers (Cloudflare, etc.). These leads go to the "review" queue.
- Increase `REQUEST_TIMEOUT` in `backend/config.py` (default: 30s)
- Lower `CONCURRENCY_LIMIT` to avoid rate limits
- Manually visit the site for blocked leads

### Kimi API errors
Kimi K2.5 is a thinking model that requires `temperature=1`. This is already configured. If you see timeout errors, the model may need more time â€” try increasing the `httpx.Timeout` in `backend/intelligence.py`.

### Stale / wrong results
```bash
cd backend && python main.py --clear-checkpoint
```
Clears the checkpoint AND output CSVs for a clean run.

### "No EXA_API_KEY found"
The Exa discovery step (`test_exa.py`) is optional. You can skip it entirely and provide your own CSV.

---

## Roadmap

- [x] Chat interface â€” guided AI conversation to define ICP and launch searches
- [x] Web-based pipeline â€” full crawl + qualify with live streaming results
- [x] Dual-LLM security â€” prompt injection defense via isolated query generation
- [x] Docker â€” containerized deployment with `docker compose up`
- [x] Industry-agnostic â€” dynamic ICP from chat, no hardcoded keywords/prompts
- [x] Multi-tenant auth â€” Supabase user accounts, JWT-protected endpoints
- [x] Full dashboard â€” stats, hunts, pipeline table, lead detail drawer
- [x] Interactive map â€” Mapbox GL with glowing dots, fly-to, popups, **live pipeline updates**
- [x] Pipeline CRM â€” lead status management (new â†’ contacted â†’ won/lost)
- [x] Chat persistence â€” full conversation saved to DB, resume any hunt from the dashboard
- [x] Live map geocoding â€” LLM extracts HQ location, built-in geocoder plots leads automatically
- [ ] Email drafting module â€” auto-generate cold emails from deep research
- [ ] CRM integrations â€” push hot leads to HubSpot, Salesforce, etc.
- [ ] Deep research in chat â€” trigger multi-page analysis from the web UI
- [ ] Team workspaces â€” shared searches and lead assignments

---

## Contributing

### Getting Started

1. Fork the repo
2. Create a feature branch (`git checkout -b feature/my-feature`)
3. Set up local dev (see [Quick Start](#option-a-local-development-recommended-for-contributors))
4. Make your changes
5. Test:
   - Backend: `cd backend && python -m pytest tests/`
   - Frontend: `cd frontend && npm run build` (type checks)
   - Manual: run both servers and exercise the changed feature
6. Submit a PR

### Architecture Notes

- **Backend** â€” Python 3.9+, FastAPI, SQLAlchemy async, asyncpg. All routes in `chat_server.py`. Database models in `db/models.py`. Auth via Supabase JWT (JWKS verification in `auth/__init__.py`).
- **Frontend** â€” Next.js 16, React 19, TypeScript 5, Tailwind CSS 4. Design tokens defined in `globals.css` (`--color-*`, `--font-*`). Dashboard pages under `src/app/dashboard/`. Auth via `@supabase/ssr`.
- **Qualification is dynamic** â€” The ICP is extracted from the userâ€™s chat conversation and passed as structured context to the LLM. No hardcoded industry prompts. If youâ€™re adding features, donâ€™t assume a specific industry.
- **Dual-LLM security** â€” Raw user input never reaches the query generation LLM. Keep this separation.
- Keep the modular architecture â€” each module should do one thing and be independently testable.

### Code Style

- Python: follow existing patterns, type hints encouraged
- TypeScript: `font-mono` for UI labels, `font-sans` for body text, `text-[10px] uppercase tracking-[0.15em]` for micro-labels
- Colours: use theme tokens (`text-primary`, `secondary`, `hot`, `review`, `surface-2`, etc.) from `globals.css`

---

## License

Private â€” Mainrich International / Hunt
